---
lab:
  title: 画像内のオブジェクトを検出する
  description: Azure AI Custom Vision サービスを使用して、物体検出モデルをトレーニングします。
---

# 画像内のオブジェクトを検出する

**Azure AI Custom Vision** サービスを使用すると、独自の画像でトレーニングされた Computer Vision モデルを作成できます。 これを使用して、*画像分類* および *物体検出* モデルをトレーニングできます。その後、公開してアプリケーションから利用できます。

この演習では、Custom Vision サービスを使用して、画像内の 3 つのクラスの果物 (リンゴ、バナナ、オレンジ) を検出して特定できる *物体検出* モデルをトレーニングします。

この演習は、Azure Custom Vision Python SDK に基づいていますが、次のような複数の言語固有の SDK を使用して Vision アプリケーションを開発することができます。

* [Azure Custom Vision for JavaScript (トレーニング)](https://www.npmjs.com/package/@azure/cognitiveservices-customvision-training)
* [Azure Custom Vision for JavaScript (予測)](https://www.npmjs.com/package/@azure/cognitiveservices-customvision-prediction)
* [Azure Custom Vision for Microsoft .NET (トレーニング)](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training/)
* [Azure Custom Vision for Microsoft .NET (予測)](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction/)
* [Azure Custom Vision for Java (トレーニング)](https://search.maven.org/artifact/com.azure/azure-cognitiveservices-customvision-training/1.1.0-preview.2/jar)
* [Azure Custom Vision for Java (予測)](https://search.maven.org/artifact/com.azure/azure-cognitiveservices-customvision-prediction/1.1.0-preview.2/jar)

この演習は約 **45** 分かかります。

## Custom Vision リソースを作成する

モデルをトレーニングする前に、*トレーニング* と *予測* のために Azure リソースが必要になります。 これらのタスクごとに **Custom Vision** リソースを作成することも、単一のリソースを作成して両方のタスクに使用することもできます。 この演習では、トレーニング用と予測用の **Custom Vision** リソースを作成します。

1. [Azure portal](https://portal.azure.com) (`https://portal.azure.com`) を開き、Azure 資格情報を使用してサインインします。 表示されているすべてのウェルカム メッセージまたはヒントを閉じます。
1. **[リソースの作成]** を選択します。
1. 検索バーで `Custom Vision` を検索し、**[Custom Vision]** を選択して、次の設定でリソースを作成します。
    - **作成オプション**: 両方
    - **[サブスクリプション]**:*ご自身の Azure サブスクリプション*
    - **リソース グループ**: *リソース グループを作成または選択します*
    - **[リージョン]**: *使用できるリージョンを選択します*
    - **[名前]**: "Custom Vision リソースの有効な名前"**
    - **トレーニング価格レベル**: F0
    - **[予測価格レベル]**: F0

1. リソースを作成し、デプロイが完了するまで待ってから、デプロイの詳細を表示します。 2 つの Custom Vision リソースがプロビジョニングされていることに注意してください。1 つはトレーニング用で、もう 1 つは予測用です。

    > **注**:各リソースには独自の*エンドポイント*と*キー*があり、コードからのアクセスを管理するために使用されます。 画像分類モデルをトレーニングするには、コードで *トレーニング* リソース (エンドポイントとキーを含む) を使用する必要があります。トレーニング済みモデルを使用して画像クラスを予測するには、コードで *予測* リソース (エンドポイントとキーを含む) を使用する必要があります。

1. リソースがデプロイされたら、リソース グループに移動して表示します。 2 つのカスタム ビジョン リソースが表示されます。1 つはサフィックスが ***-Prediction*** です。

## Custom Vision ポータルで Custom Vision プロジェクトを作成する

物体検出モデルをトレーニングするには、トレーニング リソースに基づいて Custom Vision プロジェクトを作成する必要があります。 これを行うには、Custom Vision ポータルを使用します。

1. 新しいブラウザー タブを開きます (後で戻るため、Azure portal のタブは開いたままにしておきます)。
1. 新しいブラウザー タブで、[Custom Vision ポータル](https://customvision.ai) (`https://customvision.ai`) を開きます。 メッセージが表示されたら、Azure 資格情報を使用してサインインし、サービス使用条件に同意します。
1. 次の設定で新しいプロジェクトを作成します。
    - **名前**: `Detect Fruit`
    - **説明**: `Object detection for fruit.`
    - **Resource**:"Custom Vision リソース"**
    - **プロジェクトの種類**: Object Detection
    - **ドメイン**: General
1. プロジェクトが作成され、ブラウザーで開かれるまで待ちます。

## 画像をアップロードし、タグ付けする

物体検出プロジェクトが作成されたので、画像をアップロードし、タグを付けて、モデルをトレーニングできます。

### Custom Vision ポータルで、画像をアップロードしてタグを付ける

Custom Vision ポータルには、複数の種類のオブジェクトを含む画像をアップロードし、その中の領域にタグを付けるために使用できるビジュアル ツールが含まれています。

1. ブラウザーの新しいタブで、`https://github.com/MicrosoftLearning/mslearn-ai-vision/raw/main/Labfiles/object-detection/training-images.zip` から [トレーニング画像](https://github.com/MicrosoftLearning/mslearn-ai-vision/raw/main/Labfiles/object-detection/training-images.zip)をダウンロードし、zip フォルダーを展開してその内容を表示します。 このフォルダーには果物の画像が含まれています。
1. Custom Vision ポータルの物体検出プロジェクトで、 **[画像の追加]** を選択し、抽出したフォルダーのすべての画像をアップロードします。
1. 画像がアップロードされた後、最初のものを選択して開きます。
1. 以下の画像のように、自動的に検出された領域が表示されるまで、画像内の任意の物体の上にマウス ポインターを置きます。 その後、物体を選択し、必要に応じて、それを囲む領域のサイズを変更します。

    ![物体の既定の領域のスクリーンショット。](../media/object-region.jpg)

    単に物体の周りをドラッグして領域を作成することもできます。

1. 領域で物体が囲まれたら、次に示すように、適切な物体の種類 (*apple*、*banana*、または *orange*) で新しいタグを追加します。

    ![画像内のタグが付けられた物体のスクリーンショット。](../media/object-tag.jpg)

1. 画像内の互いの物体を選択してタグを付け、必要に応じて領域のサイズを変更し、新しいタグを追加します。

    ![画像内のタグが付けられた 2 つの物体のスクリーンショット。](../media/object-tags.jpg)

1. 右側の **[>]** リンクを使用して、次の画像に移動し、その物体にタグを付けます。 その後、単に画像のコレクション全体で引き続き作業を行い、apple、banana、および orange にそれぞれタグを付けます。

1. 最後の画像のタグ付けが完了したら、**[Image Detail]** エディターを閉じます。 **[Training Images]** ページの **[Tags]** で **[Tagged]** を選択して、タグ付けされたすべての画像を表示します。

![プロジェクト内のタグ付けされた画像のスクリーンショット。](../media/tagged-images.jpg)

### Custom Vision SDK を使用して画像をアップロードする

Custom Vision ポータルの UI を使用して画像にタグを付けることができますが、多くの AI 開発チームは、画像内のタグとオブジェクト領域に関する情報を含むファイルを生成する他のツールを使用しています。 このようなシナリオでは、Custom Vision トレーニング API を使用して、タグ付けされた画像をプロジェクトにアップロードできます。

1. Custom Vision ポータルの **Training Images** ページの右上にある *設定* (&#9881;) アイコンをクリックして、プロジェクトの設定を表示します。
1. **[General]** (左側) の下で、このプロジェクトを一意に識別する **[プロジェクト ID]** に注意してください。
1. 右側の **[リソース]** の下に、**キー**と**エンドポイント**が表示されていることに注意してください。 これらは、*トレーニング* リソースの詳細です (この情報は、Azure portal でリソースを表示することでも取得できます)。
1. Azure portal が表示されているブラウザー タブに戻ります (後で戻るため、Custom Vision ポータルのタブは開いたままにしておきます)。
1. Azure portal で、ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成し、サブスクリプションにストレージがない ***PowerShell*** 環境を選択します。

    Azure portal の下部にあるペインに Cloud Shell のコマンド ライン インターフェイスが表示されます。

    > **注**: *Bash* 環境を使用するクラウド シェルを以前に作成した場合は、それを ***PowerShell*** に切り替えます。

    > **注**:ファイルを保持するストレージを選択するようにポータルから求められた場合は、**[ストレージ アカウントは不要です]** を選択し、お使いのサブスクリプションを選択して、**[適用]** を選択します。

1. Cloud Shell ツール バーの **[設定]** メニューで、**[クラシック バージョンに移動]** を選択します (これはコード エディターを使用するのに必要です)。

    **<font color="red">続行する前に、クラシック バージョンの Cloud Shell に切り替えたことを確認します。</font>**

1. Cloud Shell ペインをサイズ変更して、さらに多くの内容を表示できるようにします。

    > **ヒント**" 上部の境界線をドラッグすると、ペインのサイズを変更できます。 最小化ボタンと最大化ボタンを使用して、Cloud Shell とメイン ポータル インターフェイスを切り替えることもできます。

1. Cloud Shell 画面で、次のコマンドを入力して、この演習のコード ファイルを含む GitHub リポジトリをクローンします (コマンドを入力するか、クリップボードにコピーしてから、コマンド ラインで右クリックし、プレーンテキストとして貼り付けます)。

    ```
    rm -r mslearn-ai-vision -f
    git clone https://github.com/MicrosoftLearning/mslearn-ai-vision
    ```

    > **ヒント**: Cloudshell にコマンドを貼り付けると、出力が大量のスクリーン バッファーを占有する可能性があります。 `cls` コマンドを入力して、各タスクに集中しやすくすることで、スクリーンをクリアできます。

1. リポジトリが複製されたら、次のコマンドを使用してアプリケーション コード ファイルに移動します。

    ```
   cd mslearn-ai-vision/Labfiles/object-detection/python/train-detector
   ls -a -l
    ```

    このフォルダーには、アプリのアプリケーション構成ファイルとコード ファイルが含まれています。 また、複数の画像内の物体の境界ボックス座標を含む **tagged-images.json** ファイルと、画像を含む **/images** サブフォルダーも含まれています。

1. 次のコマンドを実行して、トレーニング用の Azure AI Custom Vision SDK パッケージとその他の必要なパッケージをインストールします。

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install -r requirements.txt azure-cognitiveservices-vision-customvision
    ```

1. 次のコマンドを入力して、アプリの構成ファイルを編集します。

    ```
   code .env
    ```

    このファイルをコード エディターで開きます。

1. コード ファイル内で、含まれている構成値を更新して、Custom Vision "トレーニング" リソースの**エンドポイント**と認証**キー**、および以前に作成した Custom Vision プロジェクトの**プロジェクト ID** を反映します。**
1. プレースホルダーを置き換えたら、コード エディター内で **Ctrl + S** コマンドを使用して変更を保存してから、**Ctrl + Q** コマンドを使用して、Cloud Shell コマンド ラインを開いたままコード エディターを閉じます。
1. Cloud Shell コマンド ラインで、次のコマンドを入力して、**tagged-images.json** ファイルを開き、**/images** サブフォルダー内の画像ファイルに関するタグ付け情報を表示します。

    ```
   code tagged-images.json
    ```
    
     JSON は、それぞれ 1 つ以上のタグ付き領域が含まれている画像の一覧を定義します。 タグ付けされた各領域には、タグ名、タグ付けされたオブジェクトを含む境界ボックスの上下の座標と幅と高さの寸法が含まれます。

    > **注**: このファイルの座標と寸法は、画像上の相対点を示しています。 たとえば、*高さ* の値が 0.7 の場合、ボックスは画像の高さの 70% であることを示します。 一部のタグ付けツールは、座標と寸法の値がピクセル、インチ、またはその他の測定単位を表す他の形式のファイルを生成します。

1. 変更を保存せずに JSON ファイルを閉じます (*Ctrl + Q* キー)。

1. Cloud Shell コマンド ラインで、次のコマンドを入力して、クライアント アプリケーションのコード ファイルを開きます。

    ```
   code add-tagged-images.py
    ```

1. コード ファイル内の次の詳細に注意してください。
    - Azure AI Custom Vision SDK の名前空間がインポートされます。
    - **Main** 関数は、構成設定を取得し、キーとエンドポイントを使用して認証済みの **CustomVisionTrainingClient** を作成します。これは、プロジェクト ID とともに使用され、プロジェクトへの**プロジェクト**参照を作成します。
    - **Upload_Images** 関数は、JSON ファイルからタグ付けされた領域情報を抽出し、それを使用して領域を含む画像のバッチを作成し、それをプロジェクトにアップロードします。

1. コード エディターを閉じ (*Ctrl + Q* キー)、次のコマンドを入力してプログラムを実行します。

    ```
   python add-tagged-images.py
    ```

1. プログラムが終了するのを待ちます。
1. Custom Vision ポータルが表示されているブラウザー タブに戻り (Azure portal の Cloud Shell が表示されているタブは開いたままにしておきます)、プロジェクトの **[トレーニング画像]** ページを表示します (必要に応じてブラウザーを更新します)。
1. いくつかの新しいタグ付き画像がプロジェクトに追加されていることを確認します。

## モデルをトレーニングしてテストする

これでプロジェクト内の画像にタグを付けたので、モデルをトレーニングする準備ができました。

1. Custom Vision プロジェクトで、**[トレーニング]** (&#9881;<sub>&#9881;</sub>) をクリックし、タグ付けされた画像を使用して物体検出モデルをトレーニングします。 **[クイック トレーニング]** オプションを選択します。
1. トレーニングが完了するまで待ちます (10 分ほどかかる場合があります)。

    > **ヒント**: Azure Cloud Shell の非アクティブ タイムアウトは 20 分間で、その後セッションは破棄されます。 トレーニングが完了するのを待っている間に、一時的に Cloud Shell に戻り、`ls` のようなコマンドを入力してセッションをアクティブに維持します。

1. トレーニングが完了したら、Custom Vision ポータルで、パフォーマンス メトリックスの "精度"、"再現率"、*mAP* を確認します。これらは、物体検出モデルの予測の正確性を測定するものであり、いずれも高い値である必要があります。****
1. ページの右上にある **[クイック テスト]** をクリックし、**[画像の URL]** ボックスに、「`https://aka.ms/test-fruit`」と入力し、*[画像のクイック テスト]* (&#10132;) ボタンをクリックします。
1. 生成された予測を表示します。

    ![物体検出のテストのスクリーンショット](../media/test-object-detection.png)

1. その後、**Quick Test** ウィンドウを閉じます。

## クライアント アプリケーションで物体検出機能を使用する

以上で、トレーニング済みモデルを公開してクライアント アプリケーションで使用する準備が整いました。

### 物体検出モデルを公開する

1. Custom Vision ポータルの **Performance** ページで、 **[&#128504; Publish]** をクリックして、トレーニング済みモデルを次の設定で公開します。
    - **モデル名**: `fruit-detector`
    - **Resources**: *前に作成した "-Prediction" で終わる**予測**リソース (トレーニング リソースでは<u>ありません</u>)。*
1. **[プロジェクト設定]** ページの左上にある *[プロジェクトギャラリー]* (👁) アイコンをクリックして、プロジェクトが一覧表示されている Custom Vision ポータルの [ホーム] ページに戻ります。
1. Custom Vision ポータルの [ホーム] ページの右上にある *設定* (&#9881;) アイコンをクリックして、Custom Vision サービスの設定を表示します。 次に、 **[リソース]** の下で、"-Prediction" で終わる *予測* リソース (トレーニング リソースでは<u>ありません</u>) を見つけて、その**キー**と**エンドポイント**の値を確認します (この情報は、Azure portal のリソースで表示して取得することもできます)。

## クライアント アプリケーションからの画像分類子を使用する

画像分類モデルを公開したので、クライアント アプリケーションからそれを使用できます。 ここでも、**C#** または **Python** のどちらを使用するかを選択できます。

1. Azure portal と Cloud Shell ペインが表示されているブラウザー タブに戻ります。
1. Cloud Shell で次のコマンドを実行して、クライアント アプリケーションのフォルダーに切り替え、含まれているファイルを表示します。

    ```
   cd ../test-detector
   ls -a -l
    ```

    このフォルダーには、アプリのアプリケーション構成ファイルとコード ファイルが含まれています。 また、モデルのテストに使用する次の **produce.jpg** 画像ファイルも含まれています。

    ![いくつかの果物の画像。](../media/produce.jpg)

1. 次のコマンドを実行して、予測用の Azure AI Custom Vision SDK パッケージとその他の必要なパッケージをインストールします。

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install -r requirements.txt azure-cognitiveservices-vision-customvision
    ```

1. 次のコマンドを入力して、アプリの構成ファイルを編集します。

    ```
   code .env
    ```

    このファイルをコード エディターで開きます。

1. 構成値を更新して、Custom Vision "<u>予測</u>" リソースの**エンドポイント**と認証**キー**、物体検出プロジェクトの**プロジェクト ID**、公開済みのモデルの名前 (*fruit-detector*) を反映します。** 変更を保存し (*Ctrl + S* キー)、コード エディターを閉じます (*Ctrl + Q* キー)。

1. Cloud Shell コマンド ラインで、次のコマンドを入力して、クライアント アプリケーションのコード ファイルを開きます。

    ```
   code test-detector.py
    ```

1. コードを確認し、次の詳細に注意します。
    - Azure AI Custom Vision SDK の名前空間がインポートされます。
    - **Main** 関数は構成設定を取得し、キーとエンドポイントを使用して認証済みの **CustomVisionPredictionClient** を作成します。
    - 予測クライアント オブジェクトは、リクエストでプロジェクト ID とモデル名を指定して、**produce.jpg** 画像の物体検出予測を取得するために使用されます。 次に、予測されたタグ付き領域が画像に描画され、結果が **output.jpg** として保存されます。
1. コード エディターを閉じ、次のコマンドを入力してプログラムを実行します。

    ```
   python test-detector.py
    ```

1. プログラムの出力を確認します。これには、画像で検出された各オブジェクトが一覧表示されています。
1. **output.jpg** という名前の画像ファイルが生成されていることに注意してください。 (Azure Cloud Shell 固有の) **download** コマンドを使用して、そのファイルをダウンロードします。

    ```
   download output.jpg
    ```

    ダウンロード コマンドを実行すると、ブラウザーの右下にポップアップ リンクが作成され、ここからファイルをダウンロードして開くことができます。 次のような画像が表示されます。

    ![検出された物体が強調表示されている画像。](../media/object-detection-output.jpg )

## リソースをクリーンアップする

このラボで作成した Azure リソースを他のトレーニング モジュールに使用していない場合は、それらを削除して、追加料金が発生しないようにすることができます。

1. `https://portal.azure.com` で Azure portal を開き、上部の検索バーで、このラボで作成したリソースを検索します。

1. [リソース] ページで **[削除]** を選択し、指示に従ってリソースを削除します。 または、リソース グループ全体を削除して、すべてのリソースを同時にクリーンアップすることもできます。
   
## 詳細

Custom Vision サービスを使用した物体検出の詳細については、[Custom Vision のドキュメント](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/)を参照してください。
